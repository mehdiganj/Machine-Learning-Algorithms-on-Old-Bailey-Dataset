{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM over trees:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing panda and numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the dataset and converting as panda framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data (filename):\n",
    "    \"\"\"\n",
    "    This function, imports the train/test data and create the attribute matrix and labels using the input data\n",
    "    \"\"\"\n",
    "    Matrix = []\n",
    "    Label = []\n",
    "    with open(filename) as f:\n",
    "\n",
    "        for line in f:\n",
    "            sample = line.split()\n",
    "            Label.append(float(sample[0]))\n",
    "            sample.pop(0)\n",
    "            row = []\n",
    "            for s in sample:\n",
    "                feature, value = s.split(':')\n",
    "                z = len(row)\n",
    "                nz = int(feature) - (z+1)\n",
    "                for i in range (nz):\n",
    "                    row.append(0)\n",
    "                row.append(float(value))\n",
    "            Matrix.append(row)\n",
    "    data =[]\n",
    "    M = max(len(row) for row in Matrix)\n",
    "    #print(\"M:\",M)\n",
    "    for row in Matrix:\n",
    "        nz = M - (len(row))\n",
    "        for i in range (nz):\n",
    "            row.append(0)\n",
    "        data.append(row)\n",
    "    Label1 = np.array(Label)\n",
    "    data1= np.array(data)\n",
    "    #print(\"aaa:\",Label1, data1.shape)\n",
    "    S1 = np.concatenate((data1, Label1[:,None]),axis=1)\n",
    "    attributes = np.arange(1, np.size(data1,1)+2)\n",
    "    #print(attributes)\n",
    "    samples = range(0,np.size(data1,0))\n",
    "    data2 = pd.DataFrame(S1, columns=attributes, index=samples)\n",
    "    #print('label',data2[6])\n",
    "\n",
    "    return data2\n",
    "    #print(\"data1:\",data1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy (D, w, b, loss_metric = \"SVM\", C = 1, sig2=0):\n",
    "    if loss_metric == \"Logestic regression\":\n",
    "        C = 1\n",
    "    \n",
    "    #print(sig2, C)\n",
    "    \n",
    "    \"\"\"\n",
    "        This function returns the accuracy of the dataset based on set D and weight and bias. \n",
    "    \"\"\"\n",
    "    if type(D) != np.ndarray:\n",
    "        D = D.to_numpy()\n",
    "    n_correct_prediction = 0\n",
    "    n_samples = np.size(D,0)\n",
    "    label_ix = np.size(D,1)\n",
    "    if loss_metric == \"SVM\":\n",
    "        loss = .5 *(np.dot(w,w) + b*b)\n",
    "    elif loss_metric == \"Logestic regression\":\n",
    "        loss = (np.dot(w,w) + b*b)/sig2\n",
    "        #print(\"loss\",loss)\n",
    "    for i in range(n_samples):\n",
    "        sample = D[i,:]\n",
    "        true_label = sample[-1]\n",
    "        xi = sample[:-1]\n",
    "        dot_product = np.dot(xi,w) + b\n",
    "        predicted_label = np.sign (dot_product)\n",
    "        if loss_metric == \"SVM\":\n",
    "            loss += np.max([0, 1.0 - true_label * dot_product])\n",
    "        elif loss_metric == \"Logestic regression\":\n",
    "            loss += np.log (1 + np.exp(-true_label * dot_product))\n",
    "        if predicted_label == true_label:\n",
    "            n_correct_prediction += 1\n",
    "    acc = n_correct_prediction/n_samples * 100\n",
    "    loss = C*loss\n",
    "    return acc, loss\n",
    "def prediction (D, w, b):\n",
    "    \"\"\"\n",
    "        This function returns the prediction of the dataset based on set D and weight and bias. \n",
    "    \"\"\"\n",
    "    if type(D) != np.ndarray:\n",
    "        D = D.to_numpy()\n",
    "    n_samples = np.size(D,0)\n",
    "    label_ix = np.size(D,1)\n",
    "    pred = []\n",
    "    for i in range(n_samples):\n",
    "        sample = D[i,:]\n",
    "        xi = sample[:-1]\n",
    "        predicted_label = np.sign (np.dot(xi,w) + b)\n",
    "        #print(predicted_label[0])\n",
    "        if predicted_label == -1.0:\n",
    "            predicted_label = [0.0]\n",
    "        pred.append([i, predicted_label[0]])\n",
    "        \n",
    "    Pred = pd.DataFrame(pred, columns=['example_id', 'label'])\n",
    "\n",
    "    return Pred\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fold database creation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_label(D):\n",
    "    x,y = D.shape\n",
    "    for i in range(x):\n",
    "        if D[y][i] ==0.0:\n",
    "            D[y][i] = -1.0\n",
    "    return (D)\n",
    "def k_fold(D,k):\n",
    "    cols = D.columns\n",
    "    D = D.to_numpy()\n",
    "    r_n, _ = D.shape\n",
    "    k_n = (r_n//5)\n",
    "    lb = (k-1)*k_n\n",
    "    if k == 5:\n",
    "        ub = r_n\n",
    "    else:\n",
    "        ub = k*k_n-1\n",
    "    \n",
    "    fk = D [lb:ub, :] \n",
    "    \n",
    "    Fk = pd.DataFrame(fk, columns=cols)\n",
    "    return Fk\n",
    "\n",
    "def import_label (D, new_feature):\n",
    "    D = D.to_numpy()\n",
    "    D = D.copy()\n",
    "    new_feature = new_feature.to_numpy()\n",
    "    labels = D[:, -1]\n",
    "    labels = labels[:,None]\n",
    "\n",
    "    D_out = np.append(new_feature, labels, axis=1)\n",
    "    \n",
    "    attributes = np.arange(1, np.size(D_out,1)+1)\n",
    "    D_out = pd.DataFrame(D_out, columns=attributes)\n",
    "    \n",
    "    return D_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the glove datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_data1 = import_data('glove.train.libsvm')\n",
    "Train_data_glove = update_label(Train_data1)\n",
    "Test_data1 = import_data('glove.test.libsvm')\n",
    "Test_data_glove = update_label(Test_data1)\n",
    "Eval_data_glove = import_data('glove.eval.anon.libsvm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the miscellaneous datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "misc_train = pd.read_csv ('misc-attributes-train.csv')\n",
    "train_samples, _ = misc_train.shape\n",
    "misc_test = pd.read_csv ('misc-attributes-test.csv')\n",
    "test_samples, _ = misc_test.shape\n",
    "misc_eval = pd.read_csv ('misc-attributes-eval.csv')\n",
    "eval_samples, _ = misc_eval.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In order to convert the database to one hot encoding, all the dataset are concatenated and converted to correlate the cominations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>defendant_age</th>\n",
       "      <th>defendant_gender</th>\n",
       "      <th>num_victims</th>\n",
       "      <th>victim_genders</th>\n",
       "      <th>offence_category</th>\n",
       "      <th>offence_subcategory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>theft</td>\n",
       "      <td>theftFromPlace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>17.0</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>theft</td>\n",
       "      <td>pocketpicking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>theft</td>\n",
       "      <td>pocketpicking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>theft</td>\n",
       "      <td>simpleLarceny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>52.0</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>theft</td>\n",
       "      <td>pocketpicking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5245</td>\n",
       "      <td>0.0</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>theft</td>\n",
       "      <td>theftFromPlace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5246</td>\n",
       "      <td>0.0</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>no_gender</td>\n",
       "      <td>sexual</td>\n",
       "      <td>sodomy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5247</td>\n",
       "      <td>0.0</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>theft</td>\n",
       "      <td>stealingFromMaster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5248</td>\n",
       "      <td>26.0</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>theft</td>\n",
       "      <td>burglary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5249</td>\n",
       "      <td>16.0</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>theft</td>\n",
       "      <td>simpleLarceny</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      defendant_age defendant_gender  num_victims victim_genders  \\\n",
       "0              62.0           female            1           male   \n",
       "1              17.0             male            1           male   \n",
       "2               0.0             male            1           male   \n",
       "3               0.0             male            1           male   \n",
       "4              52.0             male            1         female   \n",
       "...             ...              ...          ...            ...   \n",
       "5245            0.0             male            1           male   \n",
       "5246            0.0             male            0      no_gender   \n",
       "5247            0.0             male            1           male   \n",
       "5248           26.0             male            1           male   \n",
       "5249           16.0             male            1         female   \n",
       "\n",
       "     offence_category offence_subcategory  \n",
       "0               theft      theftFromPlace  \n",
       "1               theft       pocketpicking  \n",
       "2               theft       pocketpicking  \n",
       "3               theft       simpleLarceny  \n",
       "4               theft       pocketpicking  \n",
       "...               ...                 ...  \n",
       "5245            theft      theftFromPlace  \n",
       "5246           sexual              sodomy  \n",
       "5247            theft  stealingFromMaster  \n",
       "5248            theft            burglary  \n",
       "5249            theft       simpleLarceny  \n",
       "\n",
       "[25000 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database = pd.concat([misc_train, misc_test, misc_eval], axis=0)\n",
    "database[database.isnull().any(axis=1)]\n",
    "# Converting \"NaN\" to no_gender in victom_genders category:\n",
    "database = database.fillna({\"victim_genders\": \"no_gender\"})\n",
    "database.head()\n",
    "\n",
    "# convert all string data in defendant such as not known ,... to Nan and then substitute nan with 0;\n",
    "database['defendant_age'] = pd.to_numeric(database.defendant_age, errors='coerce')\n",
    "database = database.fillna({\"defendant_age\": 0})\n",
    "database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that all the data are free of Nan we can convert them to one-hot encoding.\n",
    "misc_transfered = pd.concat([database.defendant_age, database.num_victims, pd.get_dummies(database.defendant_gender), pd.get_dummies(database.victim_genders), pd.get_dummies(database.offence_category), pd.get_dummies(database.offence_subcategory)], axis=1)\n",
    "# for dicision tree i convert all of the featres to one-hot encoding\n",
    "misc_transfered_all_bin = pd.concat([pd.get_dummies(database.defendant_age), pd.get_dummies(database.num_victims), pd.get_dummies(database.defendant_gender), pd.get_dummies(database.victim_genders), pd.get_dummies(database.offence_category), pd.get_dummies(database.offence_subcategory)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_misc_transfered = misc_transfered_all_bin.iloc[:train_samples,:] \n",
    "Test_misc_transfered = misc_transfered_all_bin.iloc[train_samples:train_samples+test_samples,:]\n",
    "Eval_misc_transfered = misc_transfered_all_bin.iloc[train_samples+test_samples:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17500, 227)\n"
     ]
    }
   ],
   "source": [
    "Train_misc = import_label(Train_data_glove, Train_misc_transfered)\n",
    "Test_misc = import_label(Test_data_glove, Test_misc_transfered)\n",
    "Eval_misc = import_label(Eval_data_glove, Eval_misc_transfered)\n",
    "print(Train_misc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "      <th>227</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 227 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1    2    3    4    5    6    7    8    9    10   ...  218  219  220  221  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "2  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "3  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  1.0  0.0  0.0  0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   222  223  224  225  226  227  \n",
       "0  1.0  0.0  0.0  0.0  0.0 -1.0  \n",
       "1  0.0  0.0  0.0  0.0  0.0 -1.0  \n",
       "2  0.0  0.0  0.0  0.0  0.0  1.0  \n",
       "3  0.0  0.0  0.0  0.0  0.0  1.0  \n",
       "4  0.0  0.0  0.0  0.0  0.0 -1.0  \n",
       "\n",
       "[5 rows x 227 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_misc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SVM over trees:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def SVM (D, max_epoch, learning_rate, Cost, threshold=0.0005):\n",
    "    c = Cost\n",
    "    if type(D) != np.ndarray:\n",
    "        D = D.to_numpy()\n",
    "    lr0 = learning_rate\n",
    "    #print('training size:')\n",
    "     \n",
    "    w_size = np.size(D,1)-1\n",
    "    w = -.01 + 0.02 * np.random.rand(w_size)\n",
    "    #w = -.01 * np.ones(w_size)\n",
    "    \n",
    "    b = -.01 + 0.02 * np.random.rand(1)\n",
    "    #b = -0.01\n",
    "    update = 0\n",
    "    ep_w = []\n",
    "    ep_b = []\n",
    "    ep_update = []\n",
    "    train_loss = []\n",
    "    losses = []\n",
    "    j = 0\n",
    "    for epoch in range(1, max_epoch+1):\n",
    "        #1.shuffle the data\n",
    "        lr = lr0/(1+epoch)\n",
    "        np.random.shuffle(D)\n",
    "        #2.Update weights:\n",
    "        for i in range (np.size(D,0)):\n",
    "            xi = D[i,:-1]\n",
    "            yi = D[i,-1]\n",
    "            if yi * (np.dot(xi, w) + b) <= 1:\n",
    "                update += 1\n",
    "                w = (1-lr)*w + lr * c * yi * xi\n",
    "                b = (1-lr)*b + lr * c * yi\n",
    "            else:\n",
    "                w = (1-lr)*w\n",
    "                b = (1-lr)*b \n",
    "        \n",
    "        #print(\"w0:\", w[0])\n",
    "        w1 = w\n",
    "        b1 = b\n",
    "        update1 = update\n",
    "        _, loss = accuracy (D, w,b, \"SVM\", c)\n",
    "        loss = loss[0]\n",
    "        if epoch == 1:\n",
    "            loss1 = loss\n",
    "        train_loss.append(loss/loss1)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # Stopping criteria:\n",
    "        if epoch> 5:\n",
    "            #print(epoch)\n",
    "            #print(train_loss)\n",
    "            if (abs(train_loss[epoch-1] - train_loss[epoch-2])) < threshold and (abs(train_loss[epoch-2] - train_loss[epoch-3])) < threshold and (abs(train_loss[epoch-3] - train_loss[epoch-4])) < threshold and (abs(train_loss[epoch-4] - train_loss[epoch-5])) < threshold:\n",
    "                j = 1\n",
    "                #print(j) \n",
    "                #print(i)\n",
    "                break\n",
    "        #print(b)\n",
    "        #print(b1)\n",
    "        ep_w.append(w1.copy())\n",
    "        ep_b.append(b1.copy())\n",
    "        ep_update.append(update1)\n",
    "        #print('ep_b:',ep_b)\n",
    "    #print('update:', update)\n",
    "    ep_w = np.array(ep_w)\n",
    "    ep_b = np.array(ep_b)\n",
    "    ep_update = np.array(ep_update)\n",
    "    return ep_w, ep_b, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    \"\"\"\n",
    "    This class contains all the functions needed for creating decision tree.\n",
    "    It gets the dataset and measure index which can be \"Entropy\" or \"Gini\" and trains the decision tree.\n",
    "    \"\"\"\n",
    "    def __init__(self, S,  Index):\n",
    "        self.label_ix = np.size(S,1)\n",
    "        #print('label_ix',np.size(S,1) )\n",
    "        self.attributes_ix = np.arange(1,np.size(S,1))\n",
    "        self.sample_size = np.size(S,0)\n",
    "        self.measure = Index\n",
    "        self.S = S\n",
    "    \n",
    "    def entropy(self, S):\n",
    "        \"\"\"\n",
    "        Returns the Entropy measure of label array\n",
    "        \"\"\"\n",
    "        tot_samples = np.size(S,0)\n",
    "        \n",
    "        if tot_samples == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            p_plus = np.size(S[S[:,-1]==+1],0) / tot_samples\n",
    "            p_min = 1 - p_plus\n",
    "            if p_plus == 0:\n",
    "                return 0\n",
    "            elif p_min == 0:\n",
    "                return 0\n",
    "            else:\n",
    "                return - p_plus * np.log2(p_plus) - p_min * np.log2(p_min)\n",
    "    \n",
    "    def gini (self, S):\n",
    "        \"\"\"\n",
    "        Returns the Gini measure of label array\n",
    "        \"\"\"\n",
    "        tot_samples = np.size(S,0)\n",
    "        if tot_samples == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            p_plus = np.size(S[S[:,-1]==+1],0) / tot_samples\n",
    "            p_min = 1 - p_plus\n",
    "            return p_plus * (1- p_plus) + p_min * (1- p_min)\n",
    "    \n",
    "    \n",
    "    def info_gain (self, S, A):\n",
    "        \"\"\"\n",
    "        This function is going to calculate the information Gain for each set and attribute \n",
    "        using Entropy or Gini measure functions\n",
    "        \"\"\"\n",
    "        nS = np.size(S,0)\n",
    "        #S_0 = pd.DataFrame() ;\n",
    "        #S_1 = pd.DataFrame() ;\n",
    "        S_0 = S [S[:,A]==0]\n",
    "        #S_0.reset_index(inplace = True, drop = True)\n",
    "        nS_0 = np.size(S_0,0)\n",
    "        S_1 = S [S[:,A]==1]\n",
    "        #S_1.reset_index(inplace = True, drop = True)\n",
    "        nS_1 = np.size(S_1,0)\n",
    "\n",
    "        if (self.measure == 'Entropy'):\n",
    "            gain = self.entropy(S) - (nS_0/nS) * self.entropy(S_0)  - (nS_1/nS) * self.entropy(S_1)\n",
    "        else:\n",
    "            if (self.measure == 'Gini'):\n",
    "                gain = self.gini(S) - (nS_0/nS) * self.gini(S_0) - (nS_1/nS) * self.gini(S_1)\n",
    "        return gain\n",
    "       \n",
    "    \n",
    "    def common_label(self,S):\n",
    "        \"\"\"\n",
    "        Create common label for set S:\n",
    "        \"\"\"\n",
    "        \n",
    "        label = np.unique(S[:,-1])[np.argmax(np.unique(S[:,-1],return_counts=True)[1])]\n",
    "    \n",
    "        return label\n",
    "    \n",
    "        \n",
    "    def ID3_depthlim(self, S, attributes,max_depth, parent_common_label = None, depth = 0):\n",
    "        if depth == max_depth:\n",
    "            return self.common_label(S), depth\n",
    "        \n",
    "        if len(np.unique(S[:,-1])) == 1:\n",
    "            return np.unique(S[:,-1])[0], depth\n",
    "        \n",
    "        if S.size == 0:\n",
    "            return parent_node_class, depth\n",
    "        else:\n",
    "            common_label = self.common_label(S)\n",
    "\n",
    "            gain_vals = [self.info_gain(S, a) for a in attributes]\n",
    "            best_attribute = attributes[np.argmax(gain_vals)]\n",
    "\n",
    "            tree = {best_attribute:{}}\n",
    "\n",
    "            attributes = [i for i in attributes if i != best_attribute]\n",
    "            \n",
    "            d =[]\n",
    "            #print((S[:,best_attribute]))\n",
    "\n",
    "            for v in np.unique(S[:,best_attribute]):\n",
    "                v = v\n",
    "                #print(v)\n",
    "                S_v = S[S[:,best_attribute] == v]\n",
    "                #print(S_v)\n",
    "                \n",
    "                #print(best_attribute)\n",
    "\n",
    "                sub_tree, dd = self.ID3_depthlim(S_v,attributes, max_depth, common_label, depth+1)\n",
    "                d.append (dd)\n",
    "\n",
    "                tree[best_attribute][v] = sub_tree\n",
    "            #print(d)   \n",
    "            depth = np.max(d)\n",
    "            return tree, depth\n",
    "    \n",
    "    \n",
    "    def predict(self, sample, attributes, tree):\n",
    "        \"\"\"\n",
    "        This function returns the predicted label of a sample based on the trained tree\n",
    "        \"\"\"\n",
    "        if tree == 1.0 or tree == -1.0:\n",
    "            #print('tree',tree)\n",
    "            return tree\n",
    "        else:   \n",
    "            for attribute in attributes:\n",
    "                if [attribute] == list(tree.keys()):\n",
    "                    at_val = sample[attribute]\n",
    "                    #print( [attribute])\n",
    "                    label = self.predict (sample, attributes, tree [attribute][at_val])\n",
    "                    return label\n",
    "                \n",
    "    def accuracy (self, S, tree):\n",
    "        \"\"\"\n",
    "        This function returns the accuracy of the trained tree based on set S. \n",
    "        \"\"\"\n",
    "        n_samples = np.size(S,0)\n",
    "        #print(n_samples)\n",
    "        label_ix = np.size(S,1)-1\n",
    "        attributes_ix = np.arange(0,np.size(S,1)-1)\n",
    "        #print(attributes_ix)\n",
    "        n_correct_prediction = 0\n",
    "        for i in range(n_samples):\n",
    "            sample = S[i,:]\n",
    "            #print(sample)\n",
    "            true_label = sample[label_ix]\n",
    "            #print(true_label)\n",
    "            predicted_label = self.predict(sample, attributes_ix, tree)\n",
    "            #print (predicted_label)\n",
    "            if predicted_label == true_label:\n",
    "                n_correct_prediction += 1\n",
    "        acc = n_correct_prediction/n_samples * 100\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the folded datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trees construction:\n",
    "def tree_const (S, tree_n, max_depth):\n",
    "    if type(S) != np.ndarray:\n",
    "        S = S.to_numpy()\n",
    "    trees = []\n",
    "    for i in range(tree_n):\n",
    "        np.random.shuffle(S)\n",
    "        S1 = S\n",
    "        n,_ = S1.shape\n",
    "        S2 = S1[1:n//10,:]\n",
    "        #print(S2)\n",
    "        Training_class = DecisionTree(S2, 'Gini')\n",
    "        attributes_ix = np.arange(0,np.size(S2,1)-1)\n",
    "        tree_i, _ = Training_class.ID3_depthlim(S2, attributes_ix, max_depth)\n",
    "        trees.append (tree_i)\n",
    "    return(trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_transform (S, trees):\n",
    "    if type(S) != np.ndarray:\n",
    "        S = S.to_numpy()\n",
    "    attributes_ix = np.arange(0,np.size(S,1)-1)\n",
    "    TC = DecisionTree(S, 'Gini')\n",
    "    phi = np.zeros((np.size(S,0), len(trees)+1))\n",
    "    i = 0\n",
    "    for sample in S:\n",
    "        j = 0\n",
    "        for tree in trees:\n",
    "            phi[i,j] = TC.predict(sample, attributes_ix, tree)\n",
    "            j += 1\n",
    "        i+= 1\n",
    "    phi[:,-1] = S[:, -1]\n",
    "    return phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_svm_tree(f1, f2, f3, f4, f5, max_epoch, learning_rate, max_depth, C, max_trees):\n",
    "    \"\"\"\n",
    "    The function calculates the mean accuracy and std based on the 5-fold cross validation\n",
    "    \"\"\"\n",
    "\n",
    "    #train_data = pd.DataFrame(columns = f1.columns)\n",
    "    dataset = []\n",
    "    acc = []\n",
    "    loss = []\n",
    "    learning_strategy='SVM'\n",
    "    for i in range (1,6):\n",
    "        valid_data = eval(\"f\"+str(i))\n",
    "        train_name =[]\n",
    "        val_name = [\"f\"+str(i)]\n",
    "        #print(i,val_name)\n",
    "        #print(valid_data)\n",
    "        for j in range(1,6):\n",
    "            if j != i:\n",
    "                #print(j)\n",
    "                train_name.append (\"f\"+str(j))\n",
    "                dataset.append(eval(\"f\"+str(j)))\n",
    "        train_data = np.concatenate(dataset)\n",
    "        dataset = []\n",
    "        #print(train_data)\n",
    "        trees = tree_const (train_data, max_trees, max_depth)\n",
    "        train_transfered = dataset_transform (train_data, trees)\n",
    "        valid_transfered = dataset_transform (valid_data, trees)\n",
    "        w, b, _ = SVM (train_transfered, max_epoch, learning_rate, C)\n",
    "        w = w[-1]\n",
    "        #print(w)\n",
    "        b = b [-1]\n",
    "        \n",
    "        #print(train_name)\n",
    "        ac, lo = accuracy (valid_transfered, w, b, learning_strategy, C, C)\n",
    "        acc.append (ac)\n",
    "        loss.append (lo)\n",
    "    #print(\"accuracy:\", acc)\n",
    "    Mean_acc = np.mean(acc)\n",
    "    Mean_loss = np.mean(loss)\n",
    "    \n",
    "    return Mean_acc, Mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_svm_tree1(f1, f2, f3, f4, f5, max_epoch, learning_rate, C):\n",
    "    \"\"\"\n",
    "    The function calculates the mean accuracy and std based on the 5-fold cross validation\n",
    "    \"\"\"\n",
    "    #train_data = pd.DataFrame(columns = f1.columns)\n",
    "    dataset = []\n",
    "    acc = []\n",
    "    loss = []\n",
    "    learning_strategy='SVM'\n",
    "    for i in range (1,6):\n",
    "        valid_data = eval(\"f\"+str(i))\n",
    "        train_name =[]\n",
    "        val_name = [\"f\"+str(i)]\n",
    "        #print(i,val_name)\n",
    "        #print(valid_data)\n",
    "        for j in range(1,6):\n",
    "            if j != i:\n",
    "                #print(j)\n",
    "                train_name.append (\"f\"+str(j))\n",
    "                dataset.append(eval(\"f\"+str(j)))\n",
    "        train_data = np.concatenate(dataset)\n",
    "        dataset = []\n",
    "        #print(train_data)\n",
    "        #trees = tree_const (train_data, max_trees, max_depth)\n",
    "        #train_transfered = dataset_transform (train_data, trees)\n",
    "        #valid_transfered = dataset_transform (valid_data, trees)\n",
    "        w, b, _ = SVM (train_data, max_epoch, learning_rate, C)\n",
    "        w = w[-1]\n",
    "        #print(w)\n",
    "        b = b [-1]\n",
    "        \n",
    "        #print(train_name)\n",
    "        ac, lo = accuracy (valid_data, w, b, learning_strategy, C, C)\n",
    "        acc.append (ac)\n",
    "        loss.append (lo)\n",
    "    #print(\"accuracy:\", acc)\n",
    "    Mean_acc = np.mean(acc)\n",
    "    Mean_loss = np.mean(loss)\n",
    "    \n",
    "    return Mean_acc, Mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trees are done...\n",
      "Combination: 1 lr: 0.1 c: 1000 depth: 1 mean_acc: 61.08173110684686 mean_loss: 39432398.869728595\n",
      "Combination: 2 lr: 0.1 c: 100 depth: 1 mean_acc: 69.33624627444576 mean_loss: 413475.17481399234\n",
      "Combination: 3 lr: 0.1 c: 10 depth: 1 mean_acc: 75.74857632793042 mean_loss: 21455.683167976807\n",
      "Combination: 4 lr: 0.1 c: 1 depth: 1 mean_acc: 69.3471881762136 mean_loss: 3188.5303234837456\n",
      "Combination: 5 lr: 0.01 c: 1000 depth: 1 mean_acc: 70.98206018045974 mean_loss: 4708972.933477053\n",
      "Combination: 6 lr: 0.01 c: 100 depth: 1 mean_acc: 77.60056505940473 mean_loss: 204326.0552023374\n",
      "Combination: 7 lr: 0.01 c: 10 depth: 1 mean_acc: 78.09209978361164 mean_loss: 17728.513773482424\n",
      "Combination: 8 lr: 0.01 c: 1 depth: 1 mean_acc: 71.85161474707058 mean_loss: 3188.4582791974562\n",
      "Combination: 9 lr: 0.001 c: 1000 depth: 1 mean_acc: 78.43502715061446 mean_loss: 1817723.317455511\n",
      "Combination: 10 lr: 0.001 c: 100 depth: 1 mean_acc: 78.09209978361164 mean_loss: 159034.3018968655\n",
      "Combination: 11 lr: 0.001 c: 10 depth: 1 mean_acc: 78.09209978361164 mean_loss: 16456.61784781851\n",
      "Combination: 12 lr: 0.001 c: 1 depth: 1 mean_acc: 77.7091593516515 mean_loss: 3194.439179974085\n",
      "Combination: 13 lr: 0.0001 c: 1000 depth: 1 mean_acc: 78.68652267994938 mean_loss: 1573388.6030571957\n",
      "Combination: 14 lr: 0.0001 c: 100 depth: 1 mean_acc: 78.09209978361164 mean_loss: 153148.90608483454\n",
      "Combination: 15 lr: 0.0001 c: 10 depth: 1 mean_acc: 78.09209978361164 mean_loss: 16551.79130499347\n",
      "Combination: 16 lr: 0.0001 c: 1 depth: 1 mean_acc: 77.81776507573593 mean_loss: 3213.1220948830746\n",
      "Trees are done...\n",
      "Combination: 17 lr: 0.1 c: 1000 depth: 2 mean_acc: 64.50125668558364 mean_loss: 34063535.260450736\n",
      "Combination: 18 lr: 0.1 c: 100 depth: 2 mean_acc: 73.43396235659168 mean_loss: 331452.7610144234\n",
      "Combination: 19 lr: 0.1 c: 10 depth: 2 mean_acc: 73.72531090515659 mean_loss: 23846.576523732085\n",
      "Combination: 20 lr: 0.1 c: 1 depth: 2 mean_acc: 61.47756175233741 mean_loss: 3224.376371650942\n",
      "Combination: 21 lr: 0.01 c: 1000 depth: 2 mean_acc: 64.44944433103336 mean_loss: 6285598.925464979\n",
      "Combination: 22 lr: 0.01 c: 100 depth: 2 mean_acc: 74.6511607398032 mean_loss: 208483.33487245566\n",
      "Combination: 23 lr: 0.01 c: 10 depth: 2 mean_acc: 78.09215041032132 mean_loss: 17456.57277818332\n",
      "Combination: 24 lr: 0.01 c: 1 depth: 2 mean_acc: 76.74900257216348 mean_loss: 3197.6338857642686\n",
      "Combination: 25 lr: 0.001 c: 1000 depth: 2 mean_acc: 78.42363614093821 mean_loss: 1904710.9871021747\n",
      "Combination: 26 lr: 0.001 c: 100 depth: 2 mean_acc: 78.09215041032132 mean_loss: 157533.28624523705\n",
      "Combination: 27 lr: 0.001 c: 10 depth: 2 mean_acc: 78.09215041032132 mean_loss: 16448.077651405598\n",
      "Combination: 28 lr: 0.001 c: 1 depth: 2 mean_acc: 77.68631853999102 mean_loss: 3194.4650712235493\n",
      "Combination: 29 lr: 0.0001 c: 1000 depth: 2 mean_acc: 78.64084105662842 mean_loss: 1546618.807714432\n",
      "Combination: 30 lr: 0.0001 c: 100 depth: 2 mean_acc: 78.09215041032132 mean_loss: 153692.4896641367\n",
      "Combination: 31 lr: 0.0001 c: 10 depth: 2 mean_acc: 78.09215041032132 mean_loss: 16554.771680400652\n",
      "Combination: 32 lr: 0.0001 c: 1 depth: 2 mean_acc: 77.6748916016821 mean_loss: 3213.6096979543936\n",
      "Trees are done...\n",
      "Combination: 33 lr: 0.1 c: 1000 depth: 4 mean_acc: 67.08392275343977 mean_loss: 23139661.70461016\n",
      "Combination: 34 lr: 0.1 c: 100 depth: 4 mean_acc: 62.271030906789704 mean_loss: 556318.0569164325\n",
      "Combination: 35 lr: 0.1 c: 10 depth: 4 mean_acc: 69.02668027599722 mean_loss: 23887.213396905176\n",
      "Combination: 36 lr: 0.1 c: 1 depth: 4 mean_acc: 61.889127505817996 mean_loss: 3199.3680222727203\n",
      "Combination: 37 lr: 0.01 c: 1000 depth: 4 mean_acc: 72.68488302780386 mean_loss: 4251670.903324314\n",
      "Combination: 38 lr: 0.01 c: 100 depth: 4 mean_acc: 73.62251092148776 mean_loss: 206269.46375254393\n",
      "Combination: 39 lr: 0.01 c: 10 depth: 4 mean_acc: 78.10356591679255 mean_loss: 17491.28157351049\n",
      "Combination: 40 lr: 0.01 c: 1 depth: 4 mean_acc: 77.41195606908096 mean_loss: 3191.30241722349\n",
      "Combination: 41 lr: 0.001 c: 1000 depth: 4 mean_acc: 77.04608337075899 mean_loss: 2040486.420055712\n",
      "Combination: 42 lr: 0.001 c: 100 depth: 4 mean_acc: 78.10356591679255 mean_loss: 160288.19534297084\n",
      "Combination: 43 lr: 0.001 c: 10 depth: 4 mean_acc: 78.10356591679255 mean_loss: 16355.272732700983\n",
      "Combination: 44 lr: 0.001 c: 1 depth: 4 mean_acc: 77.86351692320257 mean_loss: 3194.035667531358\n",
      "Combination: 45 lr: 0.0001 c: 1000 depth: 4 mean_acc: 78.74942881639652 mean_loss: 1546386.098261057\n",
      "Combination: 46 lr: 0.0001 c: 100 depth: 4 mean_acc: 78.10356591679255 mean_loss: 153241.22245371796\n",
      "Combination: 47 lr: 0.0001 c: 10 depth: 4 mean_acc: 78.10356591679255 mean_loss: 16538.22657293582\n",
      "Combination: 48 lr: 0.0001 c: 1 depth: 4 mean_acc: 77.75492099783611 mean_loss: 3212.4433678752075\n",
      "Trees are done...\n",
      "Combination: 49 lr: 0.1 c: 1000 depth: 8 mean_acc: 67.48978891928307 mean_loss: 21462512.48769501\n",
      "Combination: 50 lr: 0.1 c: 100 depth: 8 mean_acc: 63.05519781161966 mean_loss: 565163.1677587398\n",
      "Combination: 51 lr: 0.1 c: 10 depth: 8 mean_acc: 71.9709190380925 mean_loss: 23191.519659539106\n",
      "Combination: 52 lr: 0.1 c: 1 depth: 8 mean_acc: 68.48479647246151 mean_loss: 3191.8346416874265\n",
      "Combination: 53 lr: 0.01 c: 1000 depth: 8 mean_acc: 69.83857591965051 mean_loss: 5027205.615685349\n",
      "Combination: 54 lr: 0.01 c: 100 depth: 8 mean_acc: 77.72626301392235 mean_loss: 195326.65331196226\n",
      "Combination: 55 lr: 0.01 c: 10 depth: 8 mean_acc: 78.10351529008288 mean_loss: 17523.812556025132\n",
      "Combination: 56 lr: 0.01 c: 1 depth: 8 mean_acc: 75.15451108479974 mean_loss: 3188.2235908460907\n",
      "Combination: 57 lr: 0.001 c: 1000 depth: 8 mean_acc: 75.67422855509737 mean_loss: 2106595.214271739\n",
      "Combination: 58 lr: 0.001 c: 100 depth: 8 mean_acc: 78.10351529008288 mean_loss: 160098.74608863337\n",
      "Combination: 59 lr: 0.001 c: 10 depth: 8 mean_acc: 78.10351529008288 mean_loss: 16410.3585065603\n",
      "Combination: 60 lr: 0.001 c: 1 depth: 8 mean_acc: 77.6347985138611 mean_loss: 3196.5132690783694\n",
      "Combination: 61 lr: 0.0001 c: 1000 depth: 8 mean_acc: 78.62363940717756 mean_loss: 1564130.1005282155\n",
      "Combination: 62 lr: 0.0001 c: 100 depth: 8 mean_acc: 78.10351529008288 mean_loss: 153257.66030374775\n",
      "Combination: 63 lr: 0.0001 c: 10 depth: 8 mean_acc: 78.10351529008288 mean_loss: 16539.573586238977\n",
      "Combination: 64 lr: 0.0001 c: 1 depth: 8 mean_acc: 77.69195770220063 mean_loss: 3214.5983937727315\n",
      "Cross validation results for different Learning rates, loss tradeoff and depth:\n",
      " Learning rate  Loss tradeoff  depth  accuracy mean     loss mean\n",
      "        0.1000         1000.0    1.0      61.081731  3.943240e+07\n",
      "        0.1000          100.0    1.0      69.336246  4.134752e+05\n",
      "        0.1000           10.0    1.0      75.748576  2.145568e+04\n",
      "        0.1000            1.0    1.0      69.347188  3.188530e+03\n",
      "        0.0100         1000.0    1.0      70.982060  4.708973e+06\n",
      "        0.0100          100.0    1.0      77.600565  2.043261e+05\n",
      "        0.0100           10.0    1.0      78.092100  1.772851e+04\n",
      "        0.0100            1.0    1.0      71.851615  3.188458e+03\n",
      "        0.0010         1000.0    1.0      78.435027  1.817723e+06\n",
      "        0.0010          100.0    1.0      78.092100  1.590343e+05\n",
      "        0.0010           10.0    1.0      78.092100  1.645662e+04\n",
      "        0.0010            1.0    1.0      77.709159  3.194439e+03\n",
      "        0.0001         1000.0    1.0      78.686523  1.573389e+06\n",
      "        0.0001          100.0    1.0      78.092100  1.531489e+05\n",
      "        0.0001           10.0    1.0      78.092100  1.655179e+04\n",
      "        0.0001            1.0    1.0      77.817765  3.213122e+03\n",
      "        0.1000         1000.0    2.0      64.501257  3.406354e+07\n",
      "        0.1000          100.0    2.0      73.433962  3.314528e+05\n",
      "        0.1000           10.0    2.0      73.725311  2.384658e+04\n",
      "        0.1000            1.0    2.0      61.477562  3.224376e+03\n",
      "        0.0100         1000.0    2.0      64.449444  6.285599e+06\n",
      "        0.0100          100.0    2.0      74.651161  2.084833e+05\n",
      "        0.0100           10.0    2.0      78.092150  1.745657e+04\n",
      "        0.0100            1.0    2.0      76.749003  3.197634e+03\n",
      "        0.0010         1000.0    2.0      78.423636  1.904711e+06\n",
      "        0.0010          100.0    2.0      78.092150  1.575333e+05\n",
      "        0.0010           10.0    2.0      78.092150  1.644808e+04\n",
      "        0.0010            1.0    2.0      77.686319  3.194465e+03\n",
      "        0.0001         1000.0    2.0      78.640841  1.546619e+06\n",
      "        0.0001          100.0    2.0      78.092150  1.536925e+05\n",
      "        0.0001           10.0    2.0      78.092150  1.655477e+04\n",
      "        0.0001            1.0    2.0      77.674892  3.213610e+03\n",
      "        0.1000         1000.0    4.0      67.083923  2.313966e+07\n",
      "        0.1000          100.0    4.0      62.271031  5.563181e+05\n",
      "        0.1000           10.0    4.0      69.026680  2.388721e+04\n",
      "        0.1000            1.0    4.0      61.889128  3.199368e+03\n",
      "        0.0100         1000.0    4.0      72.684883  4.251671e+06\n",
      "        0.0100          100.0    4.0      73.622511  2.062695e+05\n",
      "        0.0100           10.0    4.0      78.103566  1.749128e+04\n",
      "        0.0100            1.0    4.0      77.411956  3.191302e+03\n",
      "        0.0010         1000.0    4.0      77.046083  2.040486e+06\n",
      "        0.0010          100.0    4.0      78.103566  1.602882e+05\n",
      "        0.0010           10.0    4.0      78.103566  1.635527e+04\n",
      "        0.0010            1.0    4.0      77.863517  3.194036e+03\n",
      "        0.0001         1000.0    4.0      78.749429  1.546386e+06\n",
      "        0.0001          100.0    4.0      78.103566  1.532412e+05\n",
      "        0.0001           10.0    4.0      78.103566  1.653823e+04\n",
      "        0.0001            1.0    4.0      77.754921  3.212443e+03\n",
      "        0.1000         1000.0    8.0      67.489789  2.146251e+07\n",
      "        0.1000          100.0    8.0      63.055198  5.651632e+05\n",
      "        0.1000           10.0    8.0      71.970919  2.319152e+04\n",
      "        0.1000            1.0    8.0      68.484796  3.191835e+03\n",
      "        0.0100         1000.0    8.0      69.838576  5.027206e+06\n",
      "        0.0100          100.0    8.0      77.726263  1.953267e+05\n",
      "        0.0100           10.0    8.0      78.103515  1.752381e+04\n",
      "        0.0100            1.0    8.0      75.154511  3.188224e+03\n",
      "        0.0010         1000.0    8.0      75.674229  2.106595e+06\n",
      "        0.0010          100.0    8.0      78.103515  1.600987e+05\n",
      "        0.0010           10.0    8.0      78.103515  1.641036e+04\n",
      "        0.0010            1.0    8.0      77.634799  3.196513e+03\n",
      "        0.0001         1000.0    8.0      78.623639  1.564130e+06\n",
      "        0.0001          100.0    8.0      78.103515  1.532577e+05\n",
      "        0.0001           10.0    8.0      78.103515  1.653957e+04\n",
      "        0.0001            1.0    8.0      77.691958  3.214598e+03\n",
      "Best learning rate: 0.0001\n",
      "Best Cost: 1000.0\n",
      "Best depth: 4.0\n",
      " Best learning rate  Best Loss tradeoff  Best depth  Best accuracy\n",
      "             0.0001              1000.0         4.0      78.749429\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the network accuracy based on different values for learning rates and loss tradeoff:\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Learning_rates = [10**(-1), 10**(-2), 10**(-3), 10**(-4)]\n",
    "cost = [10**3, 10**(2), 10**(1), 10**(0)]\n",
    "max_depth = [1,2, 4, 8] \n",
    "max_epoch = 10\n",
    "acc_mean = []\n",
    "acc_std = []\n",
    "result = []\n",
    "max_trees = 100\n",
    "combination = 0\n",
    "for depth in max_depth:\n",
    "    # Constracting the trees using the training dataset:\n",
    "    trees = tree_const (Train_misc, max_trees, depth)\n",
    "    train_transfered = dataset_transform (Train_misc, trees)\n",
    "    print(\"Trees are done...\")\n",
    "    # Creating folds for cross validation\n",
    "    Data1 = Train_misc\n",
    "    cols = Data1.columns\n",
    "    Data1 = Data1.to_numpy()\n",
    "    np.random.shuffle(Data1)\n",
    "    Data1 = pd.DataFrame(Data1, columns=cols)\n",
    "    f1 = k_fold(Data1,1)\n",
    "    f2 = k_fold(Data1,2)\n",
    "    f3 = k_fold(Data1,3)\n",
    "    f4 = k_fold(Data1,4)\n",
    "    f5 = k_fold(Data1,5)\n",
    "    f1 = f1.to_numpy()\n",
    "    f2 = f2.to_numpy()\n",
    "    f3 = f3.to_numpy()\n",
    "    f4 = f4.to_numpy()\n",
    "    f5 = f5.to_numpy() \n",
    "    for lr in Learning_rates:\n",
    "        for c in cost:\n",
    "            mean_acc, mean_loss = cross_val_svm_tree1(f1, f2, f3, f4, f5, max_epoch, lr, c)\n",
    "            acc_mean.append(mean_acc)\n",
    "            result.append([lr, c, depth, mean_acc, mean_loss])\n",
    "            combination += 1\n",
    "            print(\"Combination:\", combination, \"lr:\", lr, \"c:\", c, \"depth:\", depth, \"mean_acc:\", mean_acc, \"mean_loss:\", mean_loss)\n",
    "\n",
    "result = np.array(result)\n",
    "Best_lr = result[np.argmax(result[:,3]), 0]\n",
    "Best_c = result[np.argmax(result[:,3]), 1]\n",
    "Best_depth = result[np.argmax(result[:,3]), 2]\n",
    "best_acc = result[np.argmax(result[:,3]), 3]\n",
    "\n",
    "print('Cross validation results for different Learning rates, loss tradeoff and depth:')\n",
    "result = pd.DataFrame(result, columns=['Learning rate', 'Loss tradeoff', 'depth', 'accuracy mean', 'loss mean'])\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(result.to_string(index = False))\n",
    "print('Best learning rate:', Best_lr)\n",
    "print('Best Cost:', Best_c)\n",
    "print('Best depth:', Best_depth)\n",
    "\n",
    "report1 = [{'Best learning rate':Best_lr, 'Best Loss tradeoff':Best_c, 'Best depth':Best_depth, 'Best accuracy':best_acc}]\n",
    "report1 = pd.DataFrame.from_records(report1)\n",
    "print(report1.to_string(index = False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best learning rate  Best Loss tradeoff  Best depth  Best accuracy\n",
      "             0.0001              1000.0         4.0      78.749429\n"
     ]
    }
   ],
   "source": [
    "result = np.array(result)\n",
    "Best_lr = result[np.argmax(result[:,3]), 0]\n",
    "Best_c = result[np.argmax(result[:,3]), 1]\n",
    "Best_depth = result[np.argmax(result[:,3]), 2]\n",
    "best_acc = result[np.argmax(result[:,3]), 3]\n",
    "\n",
    "report1 = [{'Best learning rate':Best_lr, 'Best Loss tradeoff':Best_c, 'Best depth':Best_depth, 'Best accuracy':best_acc}]\n",
    "report1 = pd.DataFrame.from_records(report1)\n",
    "print(report1.to_string(index = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch = 500\n",
    "max_trees=200\n",
    "trees = tree_const (Train_misc, max_trees, Best_depth)\n",
    "train_transfered = dataset_transform (Train_misc, trees)\n",
    "test_transfered = dataset_transform (Test_misc, trees)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b, loss = SVM (train_transfered, max_epoch, Best_lr, Best_c, threshold = 0.001)\n",
    "#print(b)\n",
    "train_acc = []\n",
    "train_acc1 =[]\n",
    "train_loss = []\n",
    "acc = [0,0,0]\n",
    "j = 0\n",
    "for i in range (len(b)):\n",
    "    #print(i)\n",
    "    #print(w[i][0])\n",
    "    acc_, loss = accuracy (train_transfered, w[i][:],b[i], \"SVM\", Best_c)\n",
    "    loss = loss[0]\n",
    "    train_acc.append (acc_)\n",
    "    acc[0] = i\n",
    "    acc[1] = acc_\n",
    "    acc[2] = loss\n",
    "    train_acc1.append(acc.copy())\n",
    "\n",
    "    \n",
    "#print(train_acc)\n",
    "\n",
    "train_acc = np.array(train_acc)\n",
    "\n",
    "best_epoch = np.argmax(train_acc) + 1\n",
    "\n",
    "\n",
    "test_acc, test_loss =  accuracy (test_transfered, w[best_epoch-1][:],b[best_epoch-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best learning rate  Best loss tradeoff  Best depth  Best cross val. acc. (%)  Best epoch  Train accuracy (%)  Test accuracy (%)\n",
      "             0.0001              1000.0         4.0                 78.749429          52           79.085714          79.911111\n"
     ]
    }
   ],
   "source": [
    "report1 = [{'Best learning rate':Best_lr, 'Best loss tradeoff':Best_c,'Best depth':Best_depth, 'Best cross val. acc. (%)':best_acc, \n",
    "            'Best epoch':best_epoch,\n",
    "            'Train accuracy (%)':train_acc1[best_epoch-1][1], \n",
    "            'Test accuracy (%)':test_acc}]\n",
    "\n",
    "report1 = pd.DataFrame.from_records(report1)\n",
    "print(report1.to_string(index = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred2 = prediction (dataset_transform (Eval_misc, trees),  w[best_epoch-1][:], b[best_epoch-1])\n",
    "#print(pred1)\n",
    "pred2.to_csv ('misc_labels.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
